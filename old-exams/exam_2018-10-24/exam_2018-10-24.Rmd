---
title: "exam-2018-with-solutions"
author: "Maximilian Pfundstein"
date: "10/15/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(bnlearn)
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
#BiocManager::install("gRain")
#BiocManager::install("gRbase")
library(gRain)
library(HMM)
knitr::opts_chunk$set(echo = TRUE)
```

# Graphical Models

```{r}

set.seed(567)
data("asia")
ind <- sample(1:5000, 4000)
tr <- asia[ind,]
te <- asia[-ind,]

```

```{r}

test_network = function(network, test_set) {
  grain = compile(as.grain(network))
  
  predictions = t(apply(test_set, 1, function(x) {
    evidence = setEvidence(grain, names(x[-2]), x[-2])
    query = querygrain(evidence)
    return(query$S)
  }))
  
  classified = apply(predictions, 1, function(x) {
    if (x[1] > 0.5)
      return("no")
    return("yes")
  })
  
  accuracy = sum(test_set$S == classified) / length(test_set$S)
  return(accuracy)
}

```

```{r, warning=FALSE}

bn_naive_bayes = model2network("[S][A|S][T|S][L|S][B|S][E|S][X|S][D|S]")
plot(bn_naive_bayes)

bn_net_10 = bn.fit(bn_naive_bayes, tr[1:10,], method = "bayes")
bn_net_20 = bn.fit(bn_naive_bayes, tr[1:20,], method = "bayes")
bn_net_50 = bn.fit(bn_naive_bayes, tr[1:50,], method = "bayes")
bn_net_100 = bn.fit(bn_naive_bayes, tr[1:100,], method = "bayes")
bn_net_1000 = bn.fit(bn_naive_bayes, tr[1:1000,], method = "bayes")
bn_net_2000 = bn.fit(bn_naive_bayes, tr[1:2000,], method = "bayes")

acc_10 = test_network(bn_net_10, te)
acc_20 = test_network(bn_net_20, te)
acc_50 = test_network(bn_net_50, te)
acc_100 = test_network(bn_net_100, te)
acc_1000 = test_network(bn_net_1000, te)
acc_2000 = test_network(bn_net_2000, te)

# Reversing the edges
bn_naive_bayes = model2network("[A][T][L][B][E][X][D][S|A:T:L:B:E:X:D]")
plot(bn_naive_bayes)

bn_net_10_rev = bn.fit(bn_naive_bayes, tr[1:10,], method = "bayes")
bn_net_20_rev = bn.fit(bn_naive_bayes, tr[1:20,], method = "bayes")
bn_net_50_rev = bn.fit(bn_naive_bayes, tr[1:50,], method = "bayes")
bn_net_100_rev = bn.fit(bn_naive_bayes, tr[1:100,], method = "bayes")
bn_net_1000_rev = bn.fit(bn_naive_bayes, tr[1:1000,], method = "bayes")
bn_net_2000_rev = bn.fit(bn_naive_bayes, tr[1:2000,], method = "bayes")

acc_10_rev = test_network(bn_net_10_rev, te)
acc_20_rev = test_network(bn_net_20_rev, te)
acc_50_rev = test_network(bn_net_50_rev, te)
acc_100_rev = test_network(bn_net_100_rev, te)
acc_1000_rev = test_network(bn_net_1000_rev, te)
acc_2000_rev = test_network(bn_net_2000_rev, te)

```

```{r}

# Accuracy stays the same and does not increase having more training points
# Even gets slightly worse for more training points
print(c(acc_10, acc_20, acc_50, acc_100, acc_1000, acc_2000))

# Here we see that the accuracy actually increases with more training points
print(c(acc_10_rev, acc_20_rev, acc_50_rev, acc_100_rev, acc_1000_rev, acc_2000_rev))

# Discussion
# The NB classifier only needs to estimate the parameters for distributions of the form
# p(C) and P(A_i|C) where C is the class variable and A_i is a predictive attribute. The
# alternative model needs to estimate p(C) and P(C|A_1,...,A_n). Therefore, it requires
# more data to obtain reliable estimates (e.g. many of the parental combinations may not
# appear in the training data and this is actually why you should use method="bayes", to
# avoid zero relative frequency counters).This may hurt performance when little learning
# data is available. This is actually observed in the experiments above. However, when
# the size of the learning data increases, the alternative model should outperform NB, because
# the latter assumes that the attributes are independent given the class whereas the former
# does not. In other words, note that p(C|A_1,...,A_n) is proportional to P(A_1,...,A_n|C) p(C)
# by Bayes theorem. NB assumes that P(A_1,...,A_n|C) factorizes into a product of factors
# p(A_i|C) whereas the alternative model assumes nothing. The NB's assumption may hurt
# performance. This can be observed in the experiments.

```


# Hidden Markov Model

```{r}

N = 10

# Defining States Z1, Z2, ..., ZN
states = paste(rep("Z", N), 1:N, sep = "")

# Defining Symbols S1, S2, ..., SN
symbols = paste(rep("S", N), 1:N, sep = "")

# Starting Probabilities
startProbs = rep(1/N, N)

# Transition Probabilities
transProbs = matrix(0, ncol = N, nrow = N)
# Staying in the current state with 0.5 probability is just die diagonal
diag(transProbs) = 0.5
# Moving to the next is also 0.5
diag(transProbs[,-1]) = 0.5
transProbs[10, 1] = 0.5

# Emission Probabilities
emissionProbs = matrix(0, ncol = N, nrow = 10)

# 0.2 For i-2 to i+2
for (i in 1:N) {
  for (j in c(3:-1)) {
    emissionProbs[((i-j)%%N)+1,i] = 0.2
  }
}

robot_hmm = initHMM(States = states,
                    Symbols = symbols,
                    startProbs = startProbs,
                    transProbs = transProbs,
                    emissionProbs = emissionProbs)

nSim = 100

simulatedStates = simHMM(robot_hmm, nSim)

```

```{r}

custom_forward = function(hmm, observations) {
  
  Z = matrix(NA, ncol=length(hmm$States), nrow=length(observations))
  
  Z[1,] = hmm$emissionProbs[, observations[1]] * hmm$startProbs
  
  for (t in 2:length(observations)) {
    Z[t, ] = hmm$emissionProbs[, observations[t]] * (Z[t-1,] %*% hmm$transProbs)
  }
  
  return(t(Z))
}

custom_backward = function(hmm, observations) {
  
  Z = matrix(NA, ncol=length(hmm$States), nrow=length(observations))
  
  Z[length(observations),] = 1
  
  for (t in ((length(observations)-1):0)) {
    for (state in 1:length(hmm$States)) {
     Z[t, state] = sum(Z[t+1,] * hmm$emissionProbs[,observations[t+1]] * transProbs[state,]) 
    }
  }
  
  return(t(Z))
}

```

```{r}

alpha = exp(forward(robot_hmm, simulatedStates$observation))
alpha_custom = custom_forward(robot_hmm, simulatedStates$observation)

alpha[,1:10]
alpha_custom[,1:10]

beta = exp(backward(robot_hmm, simulatedStates$observation))
beta_custom = custom_backward(robot_hmm, simulatedStates$observation)

beta[,1:10]
beta_custom[,1:10]

```

# State Space Models




