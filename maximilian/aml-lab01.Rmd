---
title: "Bayesian Learning - Lab 03"
author: "Anub Dikshit (anudi287) and Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: false
    number_sections: false
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(bnlearn)
library(tibble)
library(dplyr)
library(gRain)
library(caret)
#if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
#BiocManager::install("RBGL")
set.seed(42)
```

# Question 1

First we will load the data set and have a look at it.

```{r, warning=FALSE, echo=FALSE, include=FALSE}

as_tibble("asia")

```

```{r}

head(asia)

```

To show that different runs of the hill climbing algorithm yield in different results, we will use to different approaches.

1. We will create one random graph, and then run the `hc()` function multiple times, also utilising the different options offered.
2. We will use different random graphs which should give us different results.

The hill climbing algorithm is basically finding the next global maximum (or minimum), which is a really easy approach but will also not give us good results.

## Same Graph

So we will first create a random graph using the dataset.

```{r}

bayes_net1 = random.graph(colnames(asia))
bayes_net1
score(bayes_net1, asia)
plot(bayes_net1)

```

Now we will use the hill climbing algorithm to optimise our random graph

```{r}

bayes_net1_hc = hc(asia, start = bayes_net1)
bayes_net1_hc
score(bayes_net1_hc, asia)
plot(bayes_net1_hc)

```

We observe that the network has changed and has a better score. 

```{r}
bayes_net1_hc = hc(asia, start = bayes_net1_hc)

score(bayes_net1_hc, asia)

```

This time the score has not been updated, as we're *stuck* in the the same local optima.

What we can do is use the parameters `restart` and and `perturb`. According to the documentation and the [source code](https://rdrr.io/cran/bnlearn/src/R/hill-climbing.R) the parameters `perturb` specifies how many edges or nodes (in this terminology *arcs*) to change after each restart, the default is one. The parameter `restart` specifies how often to do that. This usually results in slightly better graphs. If `perturb = 1` all combinations are quickly investigated and a different for `restart` does not really matter that much any more:

```{r}

bayes_net1_hc_restart_low = hc(asia, start = bayes_net1_hc, restart = 10)
bayes_net1_hc_restart_high = hc(asia, start = bayes_net1_hc, restart = 100)
bayes_net1_hc_restart_vhigh = hc(asia, start = bayes_net1_hc, restart = 1000)

score(bayes_net1_hc_restart_low, asia)
score(bayes_net1_hc_restart_high, asia)
score(bayes_net1_hc_restart_vhigh, asia)

```

## 2) Different Graphs

If we use different random graphs in the beginning, the hill climbing algorithm will find different local optima.

```{r}

bayes_net2_1_hc = hc(asia, start = random.graph(colnames(asia)))
bayes_net2_2_hc = hc(asia, start = random.graph(colnames(asia)))

score(bayes_net2_1_hc, asia)
score(bayes_net2_2_hc, asia)

```

As we can see, they are different. If we specify the `restart` parameter again, it will eventually find the same optima, at least for this case, as this is a very small problem.

```{r}

bayes_net2_1_hc = hc(asia, start = random.graph(colnames(asia)), restart = 100)
bayes_net2_2_hc = hc(asia, start = random.graph(colnames(asia)), restart = 100)

score(bayes_net2_1_hc, asia)
score(bayes_net2_2_hc, asia)

```

# Inference

First we split into training and test, by sampling randomy.

```{r}

asia = asia %>% mutate(id = row_number())
train = asia %>% sample_frac(.8)
test = anti_join(asia, train, by = 'id')

train = select(train, -id)
test = select(test, -id)
testX = select(test, -S)
testY = test %>% select(S)

```

First we learn the network structure and then we fit the parameters. Afterwards we tranform the object into a `grain` object.

```{r}

# Network
bn_1 = iamb(train)
plot(bn_1)

# Parameters
bn_1_fit = bn.fit(bn_1, train)
bn_1_grain = as.grain(bn_1_fit)
#bn_1_compiled = compile(bn_1_grain)

```

Using the given evidence we calculate the posterior porbability for each case. We then take the posterior probability to classify the test data. As a comparison we also take the built in `predict()` function.

```{r}

## bnlearn
# bn.fit
# as.grain

## gRain
# compile
# setFindings (deprecated)
# querygrain

## Inference (from bnlearn)
# prob.table
# table
# cpdist

#querygrain(setEvidence(bn_1_grain, "S", ))

#firstRow = testX[1,]
#testRow = firstRow %>% mutate(A = "yes", T = "yes", L = "yes", B = "yes")

# predict for each data point of the test data
res = t(apply(testX, 1, FUN = function(x) {
  return(querygrain(setEvidence(bn_1_grain, names(x), x))$S)
}))

# classify
pred = apply(res, 1, FUN = function(x) {
  if (x[2] > 0.5) return("yes")
  return("no")
})

# Factorize
testY_factor = testY[,1]
pred_factor = factor(pred)

# We will use predict to compare our own inference with the build in version
pred_sol_factor = predict(bn_1_fit, "S", data = test, method="bayes-lw")

# Call to a library to calculate interesting metrics
confusionMatrix(pred_factor, testY_factor, mode="everything")

```

```{r}

confusionMatrix(pred_sol_factor, testY_factor, mode="everything")

```

As we can see, we observe the exact same results.

The true bayesian network is given by the following:

```{r}

dag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
dag_true = bn.fit(dag, train)
pred_dag_true = predict(dag_true, "S", data = test, method="bayes-lw")

confusionMatrix(pred_dag_true, testY_factor, mode="everything")

```

We see that the true network just has a slightly better F1 scoare compared to the trained versions from the data.

# Markow Blanket



# Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE, results = 'show'}

```

