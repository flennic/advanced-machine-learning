---
title: "Bayesian Learning - Lab 03"
author: "Anub Dikshit (anudi287) and Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: false
    number_sections: false
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(bnlearn)
library(tibble)
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
#BiocManager::install("RBGL")
set.seed(42)
```

# Question 1

First we will load the data set and have a look at it.

```{r}

as_tibble("asia")
asia

```

To show that different runs of the hill climbing algorithm yield in different results, we will use to different approaches.

1. We will create one random graph, and then run the `hc()` function multiple times, also utilising the different options offered.
2. We will use different random graphs which should give us different results.

The hill climbing algorithm is basically finding the next global maximum (or minimum), which is a really easy approach but will also not give us good results.

## 1) Same graph

So we will first create a random graph using the dataset.

```{r}

bayes_net1 = random.graph(colnames(asia))
bayes_net1
score(bayes_net1, asia)
plot(bayes_net1)

```

Now we will use the hill climbing algorithm to optimise our random graph

```{r}

bayes_net1_hc = hc(asia, start = bayes_net1)
bayes_net1_hc
score(bayes_net1_hc, asia)
plot(bayes_net1_hc)

```

We observe that the network has changed and has a better score. 

```{r}
bayes_net1_hc = hc(asia, start = bayes_net1_hc)

score(bayes_net1_hc, asia)

```

This time the score has not been updated, as we're *stuck* in the the same local optima.

What we can do is use the parameters `restart` and and `perturb`. According to the documentation and the [source code](https://rdrr.io/cran/bnlearn/src/R/hill-climbing.R) the parameters `perturb` specifies how many edges or nodes (in this terminology *arcs*) to change after each restart, the default is one. The parameter `restart` specifies how often to do that. This usually results in slightly better graphs. If `perturb = 1` all combinations are quickly investigated and a different for `restart` does not really matter that much any more:

```{r}

bayes_net1_hc_restart_low = hc(asia, start = bayes_net1_hc, restart = 10)
bayes_net1_hc_restart_high = hc(asia, start = bayes_net1_hc, restart = 100)
bayes_net1_hc_restart_vhigh = hc(asia, start = bayes_net1_hc, restart = 1000)

score(bayes_net1_hc_restart_low, asia)
score(bayes_net1_hc_restart_high, asia)
score(bayes_net1_hc_restart_vhigh, asia)

```

## 2) Different graphs

If we use different random graphs in the beginning, the hill climbing algorithm will find different local optima.

```{r}

bayes_net2_1_hc = hc(asia, start = random.graph(colnames(asia)))
bayes_net2_2_hc = hc(asia, start = random.graph(colnames(asia)))

score(bayes_net2_1_hc, asia)
score(bayes_net2_2_hc, asia)

```

As we can see, they are different. If we specify the `restart` parameter again, it will eventually find the same optima, at least for this case, as this is a very small problem.

```{r}

bayes_net2_1_hc = hc(asia, start = random.graph(colnames(asia)), restart = 100)
bayes_net2_2_hc = hc(asia, start = random.graph(colnames(asia)), restart = 100)

score(bayes_net2_1_hc, asia)
score(bayes_net2_2_hc, asia)

```


# Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE, results = 'show'}

```

